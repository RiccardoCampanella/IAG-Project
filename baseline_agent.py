import numpy as np
from enum import Enum
from typing import Dict, List, Set, Optional
from dataclasses import dataclass
from ontology_service import OntologyService
from llm_service import LLMService
import logging
from arguments_examples import argument_examples

class AgentState(Enum):
    IDLE = 0
    INPUT_PROCESSING = 1
    INFORMATION_GATHERING = 2
    EVIDENCE_ANALYSIS = 3
    REASONING = 4
    RECOMMENDATION_FORMULATION = 5
    OUTPUT_GENERATION = 6
    SELF_EVALUATION = 7
    LEARNING = 8
    ERROR = 9

@dataclass
class Plan:
    steps: List[AgentState]
    current_step: int = 0
    success_criteria: Dict[str, float] = None
    is_suspended = False
    
    def next_step(self) -> Optional[AgentState]:
        if self.current_step < len(self.steps):
            state = self.steps[self.current_step]
            self.current_step += 1
            return state
        return None

    def reset(self):
        self.current_step = 0

@dataclass
class Goal:
    description: str
    conditions: Dict[AgentState, AgentState]  # {prior_state : current_state}
    plan: Plan
    is_active: bool = False
    is_dropped: bool = False
    is_achieved: bool = False
    is_achievable: bool = False
    is_suspended: bool = False
    is_replanned: bool = False
    
    def __hash__(self):
        return hash(self.description)


logging.basicConfig(
    level=logging.DEBUG,  # Set the logging level to DEBUG to capture all types of log messages
    format='%(asctime)s - %(levelname)s - %(message)s',  # Specify the log message format
    handlers=[logging.StreamHandler()]  # Output log messages to the console
)
logger = logging.getLogger(__name__)

class FakeNewsAgent:

    def __init__(self, ontology_service: OntologyService=None, llm_service: LLMService=None):
        self.ontology_service = ontology_service
        self.llm_service = llm_service
        self.analysis_results = argument_examples #TODO remove these variables when done with testing.
        self.current_news_item = "Eating spicy food causes hair loss" #TODO remove these variables when done with testing.
        self.state = AgentState.IDLE
        self.initialise_goals()
        self.agent_memory = self.subgoals.copy()
        self.hyperparameters = self.initialise_hyperparameters()


    def initialise_goals(self):
        """Initialize the main goal and subgoals with proper plans."""
        self.main_goal = "Perform fact-checking and provide return statement"
        
        # this plan is hardcoded  (will be generated by planner and llm for the advanced Agent)
        standard_plan = Plan(steps=[
            AgentState.INPUT_PROCESSING,
            AgentState.INFORMATION_GATHERING,
            AgentState.EVIDENCE_ANALYSIS,
            AgentState.REASONING,
            AgentState.RECOMMENDATION_FORMULATION,
            AgentState.SELF_EVALUATION,
            AgentState.LEARNING,
            AgentState.ERROR
        ])

        # the plan of the main goal is broken down into subgoals with a subplan each (will be generated by planner and llm fdor the advanced Agent)
        self.subgoals: Set[Goal] = {
            Goal(
                description="Process and validate input",
                conditions={AgentState.IDLE : AgentState.INPUT_PROCESSING},
                plan=Plan(steps=[AgentState.INPUT_PROCESSING])
            ),
            Goal(
                description="Query the Ontology for relevant information",
                conditions={AgentState.INPUT_PROCESSING : AgentState.INFORMATION_GATHERING},
                plan=Plan(steps=[AgentState.INFORMATION_GATHERING])
            ),
            Goal(
                description="Query LLM for analysis",
                conditions={AgentState.INPUT_PROCESSING : AgentState.INFORMATION_GATHERING},
                plan=Plan(steps=[AgentState.INFORMATION_GATHERING])
            ),
            Goal(
                description="Compare and Analyze evidence",
                conditions={AgentState.INFORMATION_GATHERING : AgentState.EVIDENCE_ANALYSIS},
                plan=Plan(steps=[AgentState.EVIDENCE_ANALYSIS])
            ),
            Goal(
                description="Formulate recommendation",
                conditions={AgentState.EVIDENCE_ANALYSIS : AgentState.RECOMMENDATION_FORMULATION},
                plan=Plan(steps=[AgentState.RECOMMENDATION_FORMULATION])
            ),
            # Additional subgoals
            Goal(
                description="Mantain readiness to process new queries",
                conditions = {AgentState.IDLE : AgentState.IDLE},
                plan=Plan(steps=[AgentState.IDLE])
            ) 
            }
        """
        Goal(
            description="Compute Trust and Confidence Score",
            conditions={AgentState.EVIDENCE_ANALYSIS : AgentState.REASONING},
            plan=Plan(steps=[AgentState.REASONING])
            ),
        Goal(
            description="Accurately interpret and classify user input",
            conditions = {}, # adopted from any prior, current 
            plan=Plan(steps=[AgentState.INPUT_PROCESSING])
        ),
        Goal(
            description="Achieve a state of informed decision-making for the user regarding the factual accuracy of the query",
            conditions = {}, # adopted from any prior, current
            plan=Plan(steps=[AgentState.REASONING])
        ),
        Goal(
            description="Test if sufficient information has been gathered before proceeding to analysis",
            conditions = {}, # adopted from any prior, current
            plan=Plan(steps=[AgentState.INFORMATION_GATHERING])
        ),
        Goal(
            description="Test the reliability and completeness of the fact-check",
            conditions = {}, # adopted from any prior, current
            plan=Plan(steps=[AgentState.SELF_EVALUATION])
        ),
        Goal(
            description="Improve fact-checking strategies based on experience",
            conditions = {}, # adopted from any prior, current
            plan=Plan(steps=[AgentState.LEARNING])
        ),
        Goal(
            description="Manage and recover from errors in the fact-checking process",
            conditions = {}, # adopted from any prior, current
            plan=Plan(steps=[AgentState.ERROR])
        ),"""

       
    # transitioning once the prior state conditions are satisfied 
    def transition_to_state(self, new_state: AgentState) -> None:
        """Transition to a new state and handle related goal updates."""
        logger.info(f"Transitioning from {self.state} to {new_state}")
        self.agent_memory.add(frozenset(self.subgoals.copy())) # update agent memory
        self.deactivate_goals() #TODO check assumption 
        self.state = new_state
        self.activate_relevant_goals()
        self.agent_memory.add(frozenset(self.subgoals.copy()))
        if not self.get_active_goals():
            logger.info(f"Failed Transitioning from {self.state} to {new_state}")
    
    # transition to the next state specified by the state values!
    def procedural_state_transition(self) -> None:
        """Advance to the next logical state in the processing pipeline."""
        state_order = [state for state in AgentState]
        current_index = state_order.index(self.state)
        logging.info(f"following procedural_state_transition: State {current_index} to {current_index+1}")
        if current_index < len(state_order) - 1:
            self.transition_to_state(state_order[current_index + 1]) # assume the plan list is ordered in sequential logical order
        else:
            logging.info(f"Completed procedural_state_transition: State {current_index}")

    # goals with is_active set to true
    def get_active_goals(self) -> List[Goal]:
        return [goal for goal in self.subgoals if goal.is_active]
    
    def deactivate_goals(self):
        logging.info(f"Deactivating prior goals: {[goal.description for goal in self.get_active_goals()]}")
        for goal in self.subgoals:
            goal.is_active = False
            # a goal is dropped the state in its plane list is reached => still managed to carry out task (agent successfull)
            if goal.plan.steps:
                if goal.plan.steps[-1] == self.state: # assume the plan list is ordered in sequential logical order
                    goal.is_dropped = True
                    goal.is_achieved = True
            elif goal.is_achievable:
                 # if a goal is not reached it can be resumed later (unless unachievable or irrelevant)
                 goal.is_suspended = True
                 goal.plan.is_suspended = True
                 goal.plan.current_step = goal.plan.steps.index(self.state)
            else:
                 goal.is_suspended = False
                 goal.is_achievable = False
                 goal.is_dropped = True
                 goal.plan.steps = [] # Dastani paper's rule 
            
    
    # goals excluded from active goals
    def get_suspended_goals(self) -> List[Goal]:
        """Return currently suspended goals."""
        suspended_goals =  [goal for goal in self.subgoals if not goal.is_active and goal.is_suspended]
        logging.info(f"get_suspended_goals: {[goal.description for goal in suspended_goals]}")
        return suspended_goals

    # A goal is activated when the current state is included in the goals states 
    def activate_relevant_goals(self) -> None:
        """Activate goals relevant to the current state."""
        current_state_id = self.state.value
        for goal in self.subgoals:
            if goal.conditions:
                if not goal.is_suspended and not goal.plan:
                    logging.info(f"Failed to activate_relevant_goals")
                else:
                    for prior_state in goal.conditions.keys():
                        if current_state_id - 1 == prior_state.value:
                            goal.is_active = True
        logging.info(f"activate_relevant_goals: {[goal.description for goal in self.get_active_goals()]}")
            #else:
                #goal.is_active = True # can be activated with no conditions
       
    # execute active goal(s)'s plan  
    def adopt_active_goals(self) -> None:
        """Adopt plans for active goals."""
        active_goals = self.get_active_goals()
        logging.info(f"Adopt_active_goals: {[goal.description for goal in active_goals]}")
        for goal in active_goals:
            if goal.plan.steps:
                self.execute_plan(goal.plan)
                self.deactivate_goals()
            else:
                goal.is_achieved = True
                
    # the next state is determined by the current goal(s)'s plan
    def execute_plan(self, plan: Plan) -> None:
        """Execute the steps in a plan."""
        logging.info(f"execute_plan, {self.state}")
        while (next_state := plan.next_step()) is not None:
            try:
                self.execute_state_action(next_state)
            except Exception as e:
                logger.error(f"Error executing state {next_state}: {str(e)}")
                #self.transition_to_state(AgentState.SELF_EVALUATION)
                break
        
    # state-action mapping
    def execute_state_action(self, state: AgentState) -> None:
        """Execute the appropriate action for the given state."""
        action_map = {
            AgentState.IDLE : self.print_ready,
            AgentState.INPUT_PROCESSING : self.process_input,
            AgentState.INFORMATION_GATHERING : self.gather_information,
            AgentState.EVIDENCE_ANALYSIS : self.analyze_evidence,
            AgentState.REASONING : self.perform_goal_reasoning,
            AgentState.RECOMMENDATION_FORMULATION : self.formulate_recommendation,
            AgentState.SELF_EVALUATION : self.perform_self_evaluation
        }
        
        if state in action_map.keys():
            action_map[state]()
            logging.info(f"execute_state_action, state: {self.state}, action: {action_map[state]}, goals active: {[goal.description for goal in self.get_active_goals()]}")
        else:
            raise ValueError(f"No action defined for state {state}")
    
    # the agent will tune this knob to imporve itself
    def initialise_hyperparameters(self):
        hyperparameters = {
            'trust_ontology' : 0.8, # assigned by us
            'trust_llm' : 0.6, # assigned by us
            'trust_llm_vedant' : 0.7 
        }
        return hyperparameters


    ### Implementation of state-specific actions

    def process_input(self) -> None:
        """Validate and process the input news item."""
        logging.debug(f"process_input, state : {self.state}")
        if not self.current_news_item:
            raise ValueError("No news item to process")
        
        required_fields = ['title', 'content', 'source']
        if not all(field in self.current_news_item for field in required_fields):
            raise ValueError("Missing required fields in news item")
        
        self.analysis_results['processed_input'] = {
            'title': self.current_news_item['title'],
            'content_length': len(self.current_news_item['content']),
            'source': self.current_news_item['source']
        }
    
    def gather_information(self) -> None:
        """Gather information from both ontology and LLM."""
        logging.debug(f"gather_information, state : {self.state}")
        if self.ontology_service:
            ontology_results = self.ontology_service.query(self.current_news_item)
        try:
            print(dir(self.llm_service))
            llm_results = self.llm_service.query(self.current_news_item)
        except Exception as e :print(e)

        self.analysis_results['gathered_info'] = {
            'ontology_data': ontology_results,
            'llm_analysis': llm_results
        }

    def analyze_evidence(self) -> None:
        """Analyze gathered evidence."""
        logging.debug(f"analyze_evidence, state : {self.state}, goals active ")
        if 'gathered_info' not in self.analysis_results:
            raise ValueError("No gathered information to analyze")

    def perform_goal_reasoning(self) -> None:
        """Perform reasoning based on analyzed evidence."""
        logging.debug(f"perform_reasoning, state : {self.state}, goals active ")
        if 'evidence_analysis' not in self.analysis_results:
            raise ValueError("No analyzed evidence for reasoning")
        self.analysis_results['reasoning_results'] = self.reason_about_evidence()

    def formulate_recommendation(self) -> None:
        """Formulate a recommendation based on reasoning."""
        logging.debug(f"formulate_recomendation, state: {self.state}, goals active ")
        if 'reasoning_results' not in self.analysis_results:

            raise ValueError("No reasoning results for recommendation")
        
        self.analysis_results['final_output'] = {
            'verification_result': self.analysis_results['recommendation'],
            'trust_score': self.calculate_trust_score(), 
            'confidence_score': self.update_confidence_score(),
            'evidence_summary': self.summarize_evidence()
        }
        
    def perform_self_evaluation(self) -> None:
        """Perform self-evaluation of the analysis process."""
        logging.debug(f"perfrom_self_evaluation, state : {self.state}, goals active ")
        self.analysis_results['evaluation'] = {
            'process_complete': bool(self.analysis_results.get('final_output')),
            'confidence_level': self.calculate_confidence_score(),
            'areas_for_improvement': self.identify_improvements()
        }
        self.learn_from_experience()

    def print_ready(self):
        pass

    def learn_from_experience(self) -> None:
        """
        Adjust hyperparameters based on self-evaluation results and performance metrics.
        Called after perform_self_evaluation to tune the agent's behavior.
        """
        if not self.analysis_results.get('evaluation'):
            logger.warning("No evaluation results available for learning")
            return

        logger.info("Starting learning process to adjust hyperparameters")
        
        # Extract evaluation metrics
        evaluation = self.analysis_results['evaluation']
        confidence_level = evaluation.get('confidence_level', 0)
        process_complete = evaluation.get('process_complete', False)
        
        # Calculate performance metrics
        dropped_goals_ratio = len([g for g in self.subgoals if g.is_dropped]) / len(self.subgoals)
        suspended_goals_ratio = len(self.get_suspended_goals()) / len(self.subgoals)
        achieved_goals_ratio = len([g for g in self.subgoals if g.is_achieved]) / len(self.subgoals)
        
        # Analyze information source reliability
        llm_ontology_agreement = self._calculate_source_agreement()
        
        # Adjustment factors based on performance
        adjustment_factors = {
            'trust_ontology': self._calculate_ontology_adjustment(
                achieved_goals_ratio,
                suspended_goals_ratio,
                llm_ontology_agreement
            ),
            'trust_llm': self._calculate_llm_adjustment(
                achieved_goals_ratio,
                confidence_level,
                llm_ontology_agreement
            ),
            'trust_llm_vedant': self._calculate_vedant_adjustment(
                dropped_goals_ratio,
                confidence_level
            )
        }
        
        # Apply adjustments with learning rate and bounds
        learning_rate = 0.1
        for param, adjustment in adjustment_factors.items():
            current_value = self.hyperparameters.get(param, 0.5)
            new_value = current_value + (adjustment * learning_rate)
            # Ensure values stay within [0.1, 0.9] range
            new_value = max(0.1, min(0.9, new_value))
            self.hyperparameters[param] = new_value
            
            logger.info(f"Adjusted {param}: {current_value:.3f} -> {new_value:.3f}")

        # Store learning results for future reference
        self.analysis_results['learning'] = {
            'hyperparameter_adjustments': {
                param: self.hyperparameters[param] for param in adjustment_factors.keys()
            },
            'performance_metrics': {
                'achieved_goals_ratio': achieved_goals_ratio,
                'dropped_goals_ratio': dropped_goals_ratio,
                'suspended_goals_ratio': suspended_goals_ratio,
                'confidence_level': confidence_level,
                'llm_ontology_agreement': llm_ontology_agreement
            }
        }

    def _calculate_source_agreement(self) -> float:
        """
        Calculate agreement level between LLM and Ontology sources.
        Returns a value between 0 and 1.
        """
        if 'gathered_info' not in self.analysis_results:
            return 0.5
        
        gathered_info = self.analysis_results['gathered_info']
        ontology_data = gathered_info.get('ontology_data', {})
        llm_analysis = gathered_info.get('llm_analysis', {})
        
        if not ontology_data or not llm_analysis:
            return 0.5
        
        # Compare key findings between sources
        # This is a simplified comparison - extend based on your specific data structure
        try:
            agreement_score = sum(
                1 for k, v in ontology_data.items()
                if k in llm_analysis and llm_analysis[k] == v
            ) / len(ontology_data)
            return agreement_score
        except (AttributeError, ZeroDivisionError):
            return 0.5

    def _calculate_ontology_adjustment(
        self,
        achieved_ratio: float,
        suspended_ratio: float,
        source_agreement: float
    ) -> float:
        """Calculate adjustment for ontology trust based on performance metrics."""
        # Positive factors increase trust
        positive_factors = [
            achieved_ratio * 0.4,  # Weight achievement heavily
            source_agreement * 0.3  # Consider agreement with LLM
        ]
        
        # Negative factors decrease trust
        negative_factors = [
            suspended_ratio * 0.3  # Penalize suspended goals
        ]
        
        return sum(positive_factors) - sum(negative_factors)

    def _calculate_llm_adjustment(
        self,
        achieved_ratio: float,
        confidence: float,
        source_agreement: float
    ) -> float:
        """Calculate adjustment for LLM trust based on performance metrics."""
        # Positive factors increase trust
        positive_factors = [
            achieved_ratio * 0.3,
            confidence * 0.3,
            source_agreement * 0.2
        ]
        
        # Negative factors decrease trust
        negative_factors = [
            (1 - confidence) * 0.2  # Penalize low confidence
        ]
        
        return sum(positive_factors) - sum(negative_factors)

    def _calculate_vedant_adjustment(
        self,
        dropped_ratio: float,
        confidence: float
    ) -> float:
        """Calculate adjustment for Vedant-specific trust based on performance metrics."""
        # Positive factors increase trust
        positive_factors = [
            confidence * 0.4,
            (1 - dropped_ratio) * 0.3  # Reward low drop rate
        ]
        
        # Negative factors decrease trust
        negative_factors = [
            dropped_ratio * 0.3  # Penalize dropped goals
        ]
        
        return sum(positive_factors) - sum(negative_factors)

    ### Helper methods

    # confidence measure is affected by the mismatches between llm and ontology  
    def calculate_confidence_score(self) -> float:
        """Calculate the confidence score of the analysis."""
    

    def identify_improvements(self) -> List[str]:
        """Identify areas for improvement in the analysis process."""
        #TODO use self.agent_memory for this par to check where the plane gone wrong
        # areas of improvement includes the decisions taken by the agent that 
        # - lead to drop a goal because unachievable
        # - lead to suspend a goal
        # - re-generate a new plan
        # - repeat a state/action if not explicitly requets from user
        # - type of llm prompt
        # - type of ontology prompt
        # - number of intereaction with user
        #return []
        """
        Identify areas for improvement in the analysis process based on agent memory,
        goals state, and execution history.
        
        Returns:
            List[str]: List of identified areas for improvement
        """
        improvements = []
        
        # Check for unachievable goals
        dropped_goals = [goal for goal in self.subgoals if goal.is_dropped and not goal.is_achieved]
        if dropped_goals:
            improvements.append(
                f"Unachievable goals detected: {', '.join(goal.description for goal in dropped_goals)}. "
                "Consider adjusting goal conditions or required resources."
            )
        
        # Check for suspended goals
        suspended_goals = self.get_suspended_goals()
        if suspended_goals:
            improvements.append(
                f"Suspended goals found: {', '.join(goal.description for goal in suspended_goals)}. "
                "Review prerequisites and dependencies."
            )
        
        # Analyze transitions in agent memory for repetitive states
        if len(self.agent_memory) > 1:
            state_sequence = [goal for memory_state in self.agent_memory 
                            for goal in memory_state if goal.is_active]
            state_counts = {}
            for goal in state_sequence:
                if goal.plan.steps:
                    current_state = goal.plan.steps[goal.plan.current_step - 1] if goal.plan.current_step > 0 else goal.plan.steps[0]
                    state_counts[current_state] = state_counts.get(current_state, 0) + 1
                    
            # Identify repeated states
            repeated_states = {state: count for state, count in state_counts.items() 
                            if count > 1}
            if repeated_states:
                improvements.append(
                    f"Detected repeated states: {', '.join(f'{state.name}({count} times)' for state, count in repeated_states.items())}. "
                    "Review state transition logic."
                )
        
        # Check LLM and Ontology service usage
        if hasattr(self, 'llm_service') and self.llm_service:
            if self.hyperparameters.get('trust_llm', 0) < 0.7:
                improvements.append(
                    f"Low LLM trust score ({self.hyperparameters.get('trust_llm')}). "
                    "Consider improving prompt engineering or model selection."
                )
        
        if hasattr(self, 'ontology_service') and self.ontology_service:
            if self.hyperparameters.get('trust_ontology', 0) < 0.8:
                improvements.append(
                    "Low ontology trust score. Review ontology query patterns "
                    "and knowledge base completeness."
                )
        
        # Check for replanned goals
        replanned_goals = [goal for goal in self.subgoals if goal.is_replanned]
        if replanned_goals:
            improvements.append(
                f"Goals requiring replanning: {', '.join(goal.description for goal in replanned_goals)}. "
                "Review initial planning strategy."
            )
        
        # Check analysis results completeness
        if hasattr(self, 'analysis_results'):
            missing_steps = []
            expected_keys = {
                'processed_input', 'gathered_info', 'evidence_analysis',
                'reasoning_results', 'final_output', 'evaluation'
            }
            missing_keys = expected_keys - set(self.analysis_results.keys())
            if missing_keys:
                improvements.append(
                    f"Incomplete analysis steps: {', '.join(missing_keys)}. "
                    "Review process completion criteria."
                )
        
        # If no improvements identified, suggest general enhancement
        if not improvements:
            improvements.append(
                "No critical issues found. Consider enhancing knowledge base "
                "and refining confidence scoring algorithms."
            )
        
        return improvements

    ### Agent Test method

    def analyze_news_item(self, news_item: str) -> dict:
        """Main method to analyze a news item."""
        self.current_news_item = news_item
        self.analysis_results = {}
        
        try:
            self.transition_to_state(AgentState.INPUT_PROCESSING)
            
            # iterate over states and stop at the end of the cycle
            while self.state != AgentState.IDLE:
                # get active goals
                
                active_goals = self.get_active_goals()
                
                # re-initialise goals in case of fail
                if not active_goals:
                    # end of the cycle 
                    if self.state == AgentState.OUTPUT_GENERATION:
                        self.transition_to_state(AgentState.IDLE)
                    # automated plan rules failed, procedural state generation
                    else:
                        self.procedural_state_transition()
                
                # pursue goal
                self.adopt_active_goals()
                if self.state == AgentState.INFORMATION_GATHERING: exit()
               
                
            return self.analysis_results
            
        except Exception as e:
            logger.error(f"Error analyzing news item: {str(e)}")
            self.transition_to_state(AgentState.SELF_EVALUATION)
            raise

if __name__ == '__main__':
    FNA = FakeNewsAgent(OntologyService, LLMService)
    FNA.analyze_news_item('Does eating spicy food cause hair loss')
